# Advanced Backend System for zfit2

This guide provides a comprehensive overview of the enhanced backend system in zfit2, which now offers a complete API compatible with JAX, NumPy/SciPy, and SymPy, allowing for seamless switching between different computational engines.

## Table of Contents

1. [Overview](#overview)
2. [Backend Management](#backend-management)
3. [Core Array Operations](#core-array-operations)
4. [Automatic Differentiation](#automatic-differentiation)
5. [JIT Compilation](#jit-compilation)
6. [Vectorization and Parallelization](#vectorization-and-parallelization)
7. [Control Flow Operations](#control-flow-operations)
8. [PyTree Manipulation](#pytree-manipulation)
9. [Custom Gradients](#custom-gradients)
10. [Symbolic Computation with SymPy](#symbolic-computation-with-sympy)
11. [Performance Considerations](#performance-considerations)
12. [Common Pitfalls and Solutions](#common-pitfalls-and-solutions)

## Overview

The zfit2 backend system provides a consistent interface to three different computational engines:

- **JAX**: For accelerated array computing, automatic differentiation, and JIT compilation
- **NumPy/SciPy**: For standard scientific computing with broad compatibility
- **SymPy**: For symbolic mathematics and analytical calculations

This allows you to write your code once and run it with any of these backends, depending on your needs.

## Backend Management

### Setting the Backend

```python
import zfit2.backend as zb

# Set the backend globally
zb.set_backend("jax")   # Use JAX for all operations
zb.set_backend("numpy") # Use NumPy for all operations
zb.set_backend("sympy") # Use SymPy for all operations

# Get the current backend
backend = zb.get_backend()
print(f"Using {backend.name} backend")
```

### Temporary Backend Switching

Use the context manager to temporarily switch backends:

```python
from zfit2.backend.context import use_backend

# Current backend
print(zb.get_backend().name)

# Temporarily use NumPy backend
with use_backend("numpy"):
    print(zb.get_backend().name)
    # All operations use NumPy here
    
# Back to original backend
print(zb.get_backend().name)
```

### Environment Variable

You can set the default backend using the `ZFIT_BACKEND` environment variable:

```bash
# Set JAX as the default backend
export ZFIT_BACKEND=jax

# Set NumPy as the default backend
export ZFIT_BACKEND=numpy

# Set SymPy as the default backend
export ZFIT_BACKEND=sympy
```

## Core Array Operations

All backends provide a consistent interface for common array operations:

```python
import zfit2.backend as zb

# Array creation
x = zb.array([1, 2, 3])
zeros = zb.zeros((3, 3))
ones = zb.ones((3, 3))
full = zb.full((3, 3), 5.0)

# Basic math operations
y = zb.sin(x) + zb.cos(x)
z = zb.exp(x) * zb.log(x + 1)

# Linear algebra
matrix = zb.array([[1, 2], [3, 4]])
inv_matrix = zb.inv(matrix)
eigenvals, eigenvecs = zb.eigh(matrix)

# Device placement (for JAX)
with use_backend("jax"):
    # Create array on a specific device
    # (ignored in NumPy and SymPy)
    x_gpu = zb.array([1, 2, 3], device=jax.devices()[0])
```

## Automatic Differentiation

All backends support gradient computation:

```python
import zfit2.backend as zb

# Define a function
def f(x):
    return x**2 + zb.sin(x)

# Compute gradient
grad_f = zb.grad(f)
x = zb.array(2.0)
print(f"df/dx at x=2: {grad_f(x)}")

# Compute value and gradient simultaneously
val_and_grad_f = zb.value_and_grad(f)
val, grad_val = val_and_grad_f(x)
print(f"f(x) = {val}, df/dx = {grad_val}")

# Compute Hessian
hess_f = zb.hessian(f)
print(f"d²f/dx² at x=2: {hess_f(x)}")

# Compute Jacobian for vector-valued functions
def g(x):
    return zb.array([x[0]**2, x[1]**2])

jac_g = zb.jacobian(g)
x_vec = zb.array([1.0, 2.0])
print(f"Jacobian at [1, 2]: {jac_g(x_vec)}")
```

## JIT Compilation

JIT compilation is available in all backends (though only JAX provides actual compilation):

```python
import zfit2.backend as zb
import time

# Define a function
def f(x):
    return zb.sin(x) + zb.exp(-x**2)

# JIT compile the function
jitted_f = zb.jit(f)

# Compare performance
x = zb.array(2.0)

start = time.time()
for _ in range(1000):
    f(x)
regular_time = time.time() - start

start = time.time()
for _ in range(1000):
    jitted_f(x)
jit_time = time.time() - start

print(f"Regular time: {regular_time:.6f}s")
print(f"JIT time: {jit_time:.6f}s")
print(f"Speedup: {regular_time/jit_time:.2f}x")
```

With SymPy, JIT compilation provides symbolic optimization:

```python
with use_backend("sympy"):
    import sympy as sp
    
    x_sym = sp.symbols('x')
    
    # Define a function with a common subexpression
    def f(x):
        return (x**2 + 2*x + 1) * (zb.sin(x)**2 + zb.cos(x)**2)
    
    # JIT compilation performs symbolic optimization
    jitted_f = zb.jit(f)
    
    # Evaluate using the optimized version
    result = jitted_f(x_sym)
    print(f"Optimized expression: {result}")
    # Should simplify to (x+1)² since sin²(x) + cos²(x) = 1
```

## Vectorization and Parallelization

### Vectorization with vmap

```python
import zfit2.backend as zb

# Define a function that operates on scalars
def f(x, y):
    return x**2 + y**2

# Create batch data
x_batch = zb.array([1.0, 2.0, 3.0])
y_batch = zb.array([4.0, 5.0, 6.0])

# Vectorize the function
vf = zb.vmap(f, in_axes=(0, 0))

# Apply to batches
result = vf(x_batch, y_batch)
print(f"Batch result: {result}")
```

### Parallelization with pmap (JAX only)

```python
with use_backend("jax"):
    import jax
    
    # Define a function
    def f(x):
        return x**2
    
    # Create data for multiple devices
    xs = jax.device_put_replicated(zb.array(2.0), jax.devices())
    
    # Parallelize across devices
    pf = zb.pmap(f)
    
    # Apply in parallel
    result = pf(xs)
    print(f"Parallel result: {result}")
```

## Control Flow Operations

### Scan (Loop with State)

```python
import zfit2.backend as zb

# Define a function for scan
def add_mul(carry, x):
    return carry + x, carry * x

# Input data
xs = zb.array([1.0, 2.0, 3.0, 4.0])

# Forward scan
final, results = zb.scan(add_mul, 0.0, xs)
print(f"Final state: {final}")
print(f"Results: {results}")

# Reverse scan
final_rev, results_rev = zb.scan(add_mul, 0.0, xs, reverse=True)
print(f"Reverse final state: {final_rev}")
print(f"Reverse results: {results_rev}")
```

## PyTree Manipulation

PyTree operations for handling nested structures:

```python
import zfit2.backend as zb

# Create a nested structure
tree = {
    "a": zb.array([1.0, 2.0]),
    "b": (zb.array([3.0]), zb.array([4.0, 5.0])),
    "c": {
        "d": zb.array([6.0, 7.0, 8.0])
    }
}

# Map a function over all leaves
squared_tree = zb.tree_map(lambda x: x**2, tree)
print(f"First leaf squared: {squared_tree['a'][0]}")

# Flatten the tree
leaves, treedef = zb.tree_flatten(tree)
print(f"Number of leaves: {len(leaves)}")
print(f"First few leaves: {leaves[:3]}")

# Modify leaves and unflatten
doubled_leaves = [2 * leaf for leaf in leaves]
doubled_tree = zb.tree_unflatten(treedef, doubled_leaves)
print(f"First leaf doubled: {doubled_tree['a'][0]}")
```

## Custom Gradients

Defining custom gradients (primarily for JAX):

```python
with use_backend("jax"):
    import jax
    
    # 1. Custom JVP (forward-mode differentiation)
    def f(x):
        return zb.sin(x)
    
    def f_jvp(primals, tangents):
        x, = primals
        x_dot, = tangents
        y = zb.sin(x)
        y_dot = zb.cos(x) * x_dot
        return y, y_dot
    
    sin_custom = zb.custom_jvp(f)
    sin_custom.defjvp(f_jvp)
    
    # Test the custom JVP
    x = zb.array(2.0)
    value, grad_value = jax.jvp(sin_custom, (x,), (zb.array(1.0),))
    print(f"Custom JVP: value={value}, derivative={grad_value}")
    
    # 2. Custom VJP (reverse-mode differentiation)
    def g(x):
        return x**2
    
    def g_fwd(x):
        return x**2, x  # output and residual
    
    def g_bwd(residual, grad_output):
        x = residual
        return (2 * x * grad_output,)  # grad with respect to x
    
    square_custom = zb.custom_vjp(g)
    square_custom.defvjp(g_fwd, g_bwd)
    
    # Test the custom VJP
    x = zb.array(3.0)
    grad_func = jax.grad(square_custom)
    print(f"Custom VJP: value={square_custom(x)}, gradient={grad_func(x)}")
```

## Symbolic Computation with SymPy

Using the SymPy backend for symbolic computation:

```python
with use_backend("sympy"):
    import sympy as sp
    
    # Create symbolic variables
    x, y = sp.symbols('x y')
    
    # Define a function
    def f(x, y):
        return x**2 + y**2
    
    # Compute symbolic derivatives
    grad_f = zb.grad(f)
    hess_f = zb.hessian(f)
    
    # Evaluate symbolically
    symbolic_grad = grad_f(x, y)
    symbolic_hess = hess_f(x, y)
    
    print(f"Symbolic gradient: {symbolic_grad}")
    print(f"Symbolic Hessian: {symbolic_hess}")
    
    # Substitute values
    x_val, y_val = 2.0, 3.0
    numeric_grad = symbolic_grad.subs([(x, x_val), (y, y_val)])
    print(f"Numeric gradient at (2, 3): {numeric_grad}")
```

## Performance Considerations

### Backend Selection

- **JAX**: Best for large-scale numerical computations, GPU/TPU acceleration, and automatic differentiation
- **NumPy**: Best for quick prototyping and compatibility with other libraries
- **SymPy**: Best for symbolic calculations, analytical derivatives, and mathematical simplifications

### Memory Usage

JAX arrays are immutable, which may cause higher memory usage due to temporary arrays. Use JAX's `checkpoint` transformation for memory-intensive computations:

```python
with use_backend("jax"):
    # Define a memory-intensive function
    def f(x):
        for i in range(10):
            x = x**2 + x
        return x
    
    # Use checkpointing to trade computation for memory
    checkpointed_f = zb.checkpoint(f)
    result = checkpointed_f(zb.array(2.0))
```

### Mixed Backend Usage

You can mix backends for different parts of your computation:

```python
# Use SymPy for symbolic differentiation
with use_backend("sympy"):
    import sympy as sp
    x_sym = sp.symbols('x')
    def f(x):
        return x**3 + x**2
    
    symbolic_grad = zb.grad(f)(x_sym)
    print(f"Symbolic gradient: {symbolic_grad}")
    
    # Convert to a lambda function
    grad_lambda = sp.lambdify(x_sym, symbolic_grad, "numpy")

# Use JAX for numerical computation
with use_backend("jax"):
    x_values = zb.linspace(0, 2, 100)
    
    # Call the lambda function from SymPy
    import numpy as np
    gradient_values = zb.array(grad_lambda(np.array(x_values)))
    
    # Continue with JAX computations
    jitted_grad = zb.jit(lambda x: gradient_values * zb.exp(-x))
    result = jitted_grad(x_values)
```

## Common Pitfalls and Solutions

### 1. Different Random Number Generation

JAX uses explicit PRNG keys, while NumPy has global random state:

```python
import zfit2.backend as zb

# JAX-style with explicit keys
key = zb.random_split(zb.random_split(0)[0])[0]
samples = zb.normal(key=key, shape=(1000,))

# NumPy-style compatibility interface
samples = zb.normal(loc=0.0, scale=1.0, shape=(1000,))
```

### 2. Array Mutability

JAX arrays are immutable, while NumPy arrays are mutable:

```python
import zfit2.backend as zb

# This works in NumPy but not in JAX
with use_backend("numpy"):
    x = zb.array([1, 2, 3])
    x[0] = 10  # OK with NumPy
    
# With JAX, use functional updates
with use_backend("jax"):
    import jax.numpy as jnp
    x = zb.array([1, 2, 3])
    x = x.at[0].set(10)  # Functional update
```

### 3. Device Placement

In JAX, you may need to ensure arrays are on the same device:

```python
with use_backend("jax"):
    import jax
    
    # Create arrays on specific devices
    x = jax.device_put(zb.array([1, 2, 3]), jax.devices()[0])
    y = jax.device_put(zb.array([4, 5, 6]), jax.devices()[0])
    
    # This works because x and y are on the same device
    z = x + y
```

### 4. JIT Compilation Constraints

JAX JIT has some constraints on Python control flow:

```python
with use_backend("jax"):
    # This won't work with JAX JIT
    def problematic(x):
        if x[0] > 0:
            return x**2
        else:
            return x**3
    
    # This will work with JAX JIT
    def jit_friendly(x):
        return zb.where(x > 0, x**2, x**3)
    
    jitted = zb.jit(jit_friendly)
```

### 5. Symbolic Limitations

SymPy has limitations with certain operations:

```python
with use_backend("sympy"):
    # Some operations may not be supported
    try:
        result = zb.mean(zb.array([1, 2, 3]))
    except Exception as e:
        print(f"Error: {e}")
        
    # Use symbolic variables instead
    import sympy as sp
    x = sp.symbols('x')
    result = zb.sin(x) + zb.cos(x)
```

---

For more detailed information, see the API reference documentation and the examples directory.
